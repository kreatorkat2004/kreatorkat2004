{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kreatorkat2004/kreatorkat2004/blob/main/comp341_hw4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## COMP 341: Practical Machine Learning\n",
        "## Homework Assignment 4: Predicting List Prices for Houston Homes\n",
        "### Due: Thursday, October 23 at 11:59pm on Gradescope"
      ],
      "metadata": {
        "id": "_PxQhLYymYW1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the regression-based methods discussed in class, we will predict the list price of homes in the Houston area given attributes in existing listings on [Redfin](https://www.redfin.com) (an alternative to Zillow). Successful machine learning algorithms here can be used to assist real estate agents and prospective sellers in pricing their properties.\n",
        "\n"
      ],
      "metadata": {
        "id": "bGZIqMB31H27"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As always, fill in missing code following `# TODO:` comments or `####### YOUR CODE HERE ########` blocks and be sure to answer the short answer questions marked with `[WRITE YOUR ANSWER HERE]` in the text.\n",
        "\n",
        "All code in this notebook will be run sequentially so make sure things work in order! Be sure to also use good coding practices (e.g., logical variable names, comments as needed, etc), and make plots that are clear and legible.\n",
        "\n",
        "For this assignment, there will be **15 points** allocated for general coding and formatting points:\n",
        "* **5 points** for coding style\n",
        "* **5 points** for code flow (accurate results when everything is run sequentially)\n",
        "* **5 points** for additional style guidelines listed below\n",
        "\n",
        "Additional style guidelines:\n",
        "* Make sure to rename your .ipynb file to include your netid in the file name: `netid-hw4.ipynb`\n",
        "* For any TODO cell, make sure to include that cell's output in the .ipynb file that you submit. Many text editors have an option to clear cell outputs which is useful for getting a blank slate and running everything beginning-to-end, but always be sure to run the notebook before submitting and ensure that every cell has an output.\n",
        "* When displaying DataFrames, please do not include `.head()` or `.tail()` calls unless asked to. Just removing these calls will work as well, and will allow us to see both the beginning and end of your DataFrames, which help us ensure data is processed properly. Notebooks will by default show only the beginning and end, so you don't have to worry about long outputs here.\n",
        "* If column names are specified in the question, please use the specified name, and please avoid any sorting not specified in the instructions.\n",
        "* For plots, please ensure you have included axis labels, legends, and titles.\n",
        "* To format your short answer responses nicely, we recommend either **bolding** or *italicizing* your answer, or formatting it ```as a code block```.\n",
        "* Generally, please keep your notebook cells to one solution per cell, and preserve the order of the questions asked.\n",
        "* Finally, this can be harder to check/control and depends on which plotting libraries you prefer, but it would be helpful to limit the size/resolution of plot images in the notebook. Our grading platform has an upper limit on submission sizes it can display, and high-res plots are the usual culprit when submissions are hidden or truncated."
      ],
      "metadata": {
        "id": "BLU1yJdgvn48"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 0: Setup\n",
        "First, we need to import some libraries that are necessary to complete the assignment."
      ],
      "metadata": {
        "id": "HABAakAW1R8l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDRst9jwWPY3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add additional modules/libraries to import here (rather than wherever you first use them below):"
      ],
      "metadata": {
        "id": "yK-ghePE1XkY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# additional modules/libraries to import\n"
      ],
      "metadata": {
        "id": "MhuI7k9x1Yvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We provide some code to get the data files for this assignment into your workspace below. You only need to do the following steps of placing the homework files in your Google Drive once:\n",
        "1. Go to 'My Drive' in your own Google Drive\n",
        "2. Make a new folder named `comp341`\n",
        "3. For the [training data](https://drive.google.com/file/d/19dNolYwr6fQ_okUwWNZNCoWYkUNkCL5m/view?usp=share_link) as well as the [test data](https://drive.google.com/file/d/1dTqCj7B6t2HcwXLVW21BCzX4SE2CzpJ-/view?usp=share_link), you can expand the menu options and select `Organize -> Add shortcut`, then find and select your `comp341` folder. This is a convenient alternative to having to download and re-upload the files to your own drive. You should now have shortcuts to the `houston_homes.csv` as well as a `houston_homes_test.csv` file in your folder.\n",
        "\n",
        "If you run into trouble with accessing the files from the shortcut, then:\n",
        "\n",
        "4. Download the [training data](https://drive.google.com/file/d/19dNolYwr6fQ_okUwWNZNCoWYkUNkCL5m/view?usp=share_link) as well as the [test data](https://drive.google.com/file/d/1dTqCj7B6t2HcwXLVW21BCzX4SE2CzpJ-/view?usp=share_link). You should now have a file entitled `houston_homes.csv` as well as a `houston_homes_test.csv` on your computer.\n",
        "5. In the `comp341` folder you created in step 2, click `New -> File Upload` and select the two csv files from your computer."
      ],
      "metadata": {
        "id": "tr22ZFYChdon"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will mount your local Google Drive in colab so that you can read the file in (you will need to do this each time your runtime restarts)."
      ],
      "metadata": {
        "id": "BYDkdMM0iA-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# note that this command will trigger a request from google to allow colab\n",
        "# to access your files: you will need to accept the terms in order to access\n",
        "# the files this way\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# if you followed the instructions above exactly, CVA.csv should be\n",
        "# in comp341/; if your files are in a different directory\n",
        "# on your Google Drive, you will need to change the path below accordingly\n",
        "DATADIR = '/content/drive/My Drive/comp341/'"
      ],
      "metadata": {
        "id": "mBuu8sKrn_uI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that your Google Drive is mounted, you can read in the data in `houston_homes.csv` into a pandas DataFrame:"
      ],
      "metadata": {
        "id": "iu33pum_iHGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(DATADIR + \"houston_homes.csv\")"
      ],
      "metadata": {
        "id": "OxvpCDDfiNRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have already held out a portion of the full dataset to be your test set (`houston_homes_test.csv`). In fact, the `list_price` in this test set is hidden so you will not know the true list prices of these homes. You will not need to use this data until Part 4 of the homework assignment, when you use your favorite models to make predictions for this set of homes and submit them to [our Kaggle competition](https://www.kaggle.com/t/8ba216798d9f42c9bb4aa270dab061fd).\n",
        "\n",
        "**Important note:** while the performance of your predictions will not be graded in this assignment, a portion of your grade will be based off of whether you submitted predictions that can be evaluated by Kaggle (i.e., in the right format, passes basic checks)! As such, make sure to set up an account and try submitting predictions earlier rather than later.\n",
        "\n",
        "For Parts 1-3, you will only use the data in `houston_homes.csv`. In Parts 1 and 2, you will partition this data into a training and validation set so that we can evaluate how well our models perform for model selection. In Part 3, you will use cross-validation to dive deeper for one of the models."
      ],
      "metadata": {
        "id": "Mtq4HrOi997F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: divide your data into a training and validation set using an 80/20 split [2 pts]\n"
      ],
      "metadata": {
        "id": "lXy22QFh98ht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 1: Data exploration (29 pts)\n",
        "As always, it is important to dive into the data to see what is going on. Let's start with the typical check for missing values."
      ],
      "metadata": {
        "id": "C1g0dIxCExzl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: check for missing values in the training data, make a table of NaN counts per feature\n",
        "# include only features with non-zero missing value counts in your table [2 pts]\n"
      ],
      "metadata": {
        "id": "_l643Cy_EmgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Short Answer Question:** Do you think that the missing values are going to be problematic for predicting list price? Why or why not? [2 pts]"
      ],
      "metadata": {
        "id": "QlPowGRvKpO1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`[WRITE YOUR ANSWER HERE]`"
      ],
      "metadata": {
        "id": "Nirwj1gBK2uG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We know that housing markets can be vastly different by region. This data is isolated to the Houston housing market, but even within Houston, there can be fluctuation between different neighborhoods. Let's take a deeper look at the region diversity in the data."
      ],
      "metadata": {
        "id": "tPehsosxMDbf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: calculate how many houses are on the market per zipcode in your training data [1 pt]\n"
      ],
      "metadata": {
        "id": "czw6X-4vLEOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: for the 5 zipcodes with the most houses and\n",
        "# the 5 zipcodes with the least houses for sale (10 zipcodes total)\n",
        "# plot the number of houses per zipcode in the training data\n",
        "# sort the plot by the total number of houses per zipcode so that it is\n",
        "# easy to see the zipcodes with the most/least houses for sale [3 pts]\n"
      ],
      "metadata": {
        "id": "IeCBKdLYMz0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: using violin plots, plot the distribution of list prices\n",
        "# for the same 10 zipcodes you plotted earlier, in the same order\n",
        "# as your previous plot sorted by the total number of houses for sale [3 pts]\n"
      ],
      "metadata": {
        "id": "rSfHPaYsj1wr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Short Answer Question:** Based on the plots you generated above, do you think differences in zipcode distribution will affect models that predict list prices? Explain. [2 pts]"
      ],
      "metadata": {
        "id": "_OGwqDLrSETp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`[WRITE YOUR ANSWER HERE]`"
      ],
      "metadata": {
        "id": "W8t-WxBUSmai"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Short Answer Question:** Where are the most expensive homes located? [1 pt]"
      ],
      "metadata": {
        "id": "HcQpo_q6kQYh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`[WRITE YOUR ANSWER HERE]`"
      ],
      "metadata": {
        "id": "uECJbXm0kbtR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Latitude and longitude can give an even more detailed view of what places are for sale across Houston. Let's map out where these properties are for sale and their list price."
      ],
      "metadata": {
        "id": "2bWSrbjokl5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Plot each property in the training data by the latitude and longitude,\n",
        "# coloring points by their list price (you may have to change the color scale to see differences) [2 pts]\n"
      ],
      "metadata": {
        "id": "uVgEM7HhTZfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Short Answer Question:** Is latitude and longitude more / less / similarly informative than zipcode for determining list price? Explain. [2 pts]"
      ],
      "metadata": {
        "id": "2wg7QhBulPDP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`[WRITE YOUR ANSWER HERE]`"
      ],
      "metadata": {
        "id": "-KdxOBqeli2h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we make our models, let's check to see if our features are related to each other and how closely they relate to the variable that we want to predict, `list_price`. One quick way to check for these relationships is to use correlation."
      ],
      "metadata": {
        "id": "6EPkptDClohT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: calculate the correlation between numeric features in the training data\n",
        "# and display the results [2 pts]\n"
      ],
      "metadata": {
        "id": "vPoa1zYLT7Nr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Short Answer Question:** Based on the correlations, are there features you might consider removing before linear regression? Any that you think might be helpful? Explain. [2 pts]"
      ],
      "metadata": {
        "id": "xhRTDugXUHi8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`[WRITE YOUR ANSWER HERE]`"
      ],
      "metadata": {
        "id": "_1mBPP_IUggq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have done some exploration of our data (we can always do more - feel free to do more if you wish!), we should think about how we will define \"success\" for this problem. One helpful way is to come up with a baseline heuristic that we should exceed with regression models."
      ],
      "metadata": {
        "id": "JDmJsNW2mOCz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Short Answer Question:** If our goal is to reduce the error in our price predictions, design a baseline heuristic for this problem. Explain your rationale. [3 pts]"
      ],
      "metadata": {
        "id": "kCRcAgAZSuND"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`[WRITE YOUR ANSWER HERE]`"
      ],
      "metadata": {
        "id": "VCPaGHN9TTrH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: make \"predictions\" on your validation set using your baseline heuristic [2 pts]\n"
      ],
      "metadata": {
        "id": "4lmXn1EBAVY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: what is the RMSE and RMSLE of your baseline heuristic model? [2 pts]\n"
      ],
      "metadata": {
        "id": "Fg-6yPfXBDLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 2: Regression Models + Evaluation [27 pts]\n",
        "Now that we have done some of the initial data exploration and chosen at least one baseline heuristic, we are ready to build some different regression models to predict list price. In this section, we will try three different models.\n",
        "\n",
        "This time, we will not dictate which particular features you should use for downstream analysis. Looking at the data and the features you calculated the correlations for, you should decide which features might be useful for your model, realizing that you might not want to include all of them. You may also need to transform / preprocess some of them before using in the various models. Also be sure whatever you do to your training data, you also do the same to the validation set (while minimizing potential data leakage!)."
      ],
      "metadata": {
        "id": "Ovig-AQnp_No"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: use explanatory features of your choice to build a linear regression model on the training data\n",
        "# evaluate your model on the validation set by calculating RMSE\n",
        "# (hint: think carefully about your choice of features, if you need to scale, impute, etc) [7 pts]\n"
      ],
      "metadata": {
        "id": "tjpE_ThKcKMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Short Answer Question:** Which columns did you choose to omit from your feature set? Why did you exclude these columns? [3 pts]"
      ],
      "metadata": {
        "id": "cNWL1TiZe09j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`[WRITE YOUR ANSWER HERE]`"
      ],
      "metadata": {
        "id": "aVRIWx7TfBst"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Short Answer Question:** Does the linear regression model outperform the baseline heuristic you chose earlier? What does this comparison tell you? [2 pts]"
      ],
      "metadata": {
        "id": "B5Nn09_jCELq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`[WRITE YOUR ANSWER HERE]`"
      ],
      "metadata": {
        "id": "2dH_rzlKCTJm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: use explanatory features of your choice to build a lasso regression model on the training data\n",
        "# this time, evaluate your model on the validation set data by calculating RMSLE [2 pts]\n"
      ],
      "metadata": {
        "id": "pXvyO30mzh9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall that Lasso regression has a hyperparameter alpha. Tuning alpha might increase the performance of our model versus using the default values. Below we examine the effect of this hyperparameter on the training and validation error."
      ],
      "metadata": {
        "id": "WS5IPBZYq6x4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: we provide an initial set of alphas to explore (with variable increments from 0-20,000)\n",
        "# plot both the training RMSLE and the validation RMSLE for a lasso regression model as alpha changes\n",
        "# note that depending on what this initial set of alphas show you, you may want to focus on a smaller range\n",
        "# or expand to look at even larger alphas [5 pts]\n",
        "alphas = np.concatenate([np.arange(0,100,5), np.arange(100,1000,100), np.arange(1000,20000,1000)])\n"
      ],
      "metadata": {
        "id": "6oShyaOZz6QG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Short Answer Question:** Using the plot above as a guide, determine if there is an optimal alpha (or small range of alphas) for this task. Explain. [2 pts]"
      ],
      "metadata": {
        "id": "EUuXK4w00CXA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`[WRITE YOUR ANSWER HERE]`"
      ],
      "metadata": {
        "id": "kfrKlSRCrRiM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: using SVR, explore the RMSLE of these 3 kernels: 'linear', 'poly', and 'rbf' with C=100\n",
        "# output a table with the train and validation RMSLE for these 3 kernels\n",
        "# note: though there are only 3 variants, you should still avoid repeating code\n",
        "# (i.e., use a for loop or other iterable) [4 pts]\n"
      ],
      "metadata": {
        "id": "hSLwf9qnroHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Short Answer Question:** Which SVR kernel has the best performance? Give some intuition as to why you think it outperformed the other kernel choices. [2 pts]"
      ],
      "metadata": {
        "id": "5BJgyqZ1r8o4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`[WRITE YOUR ANSWER HERE]`"
      ],
      "metadata": {
        "id": "QeUN5pKBsJjw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 3: Cross-validation [22 pts]\n",
        "So far, we have been running all of our analysis using a train-validation split of 80/20 and choosing our parameters based on the performance across this train-validation split, but this might not be as generalizable and robust. Instead of tuning our hyperparameters on this specific partition, let's try using cross-validation.\n",
        "\n",
        "The next few questions will use decision tree regressor models and explore the effect of max_depth on validation error.\n"
      ],
      "metadata": {
        "id": "vfmkaBIwaqV3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Calculate the mean RMSLE using 5-fold cross-validation (CV) for a decision\n",
        "# tree regressor model with max_depth from 1-50 (in increments of 1).\n",
        "# Save these values for plotting later. [8 pts]\n"
      ],
      "metadata": {
        "id": "tQFyOrIYFDaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Build a decision tree regessor model with max_depth from 1-50 (in increments of 1)\n",
        "# on the training data without using CV.\n",
        "# Calculate the RMSLE for the training set and the validation set that you have been using\n",
        "# in the earlier parts of the assignment. Save these values for plotting later. [5 pts]\n"
      ],
      "metadata": {
        "id": "HGsb-ZwKwVsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: In the same figure, plot the mean RMSLE from CV, as well as training RMSLE and validation RMSLE from above\n",
        "# as max tree depth varies (1 to 50). [5 pts]\n"
      ],
      "metadata": {
        "id": "da3O7cNGv-_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Short Answer Question:** Based on these RMSLEs, which max_depth parameter is optimal? Explain. [2 pts]"
      ],
      "metadata": {
        "id": "e-lbh-0Wxz60"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`[WRITE YOUR ANSWER HERE]`"
      ],
      "metadata": {
        "id": "MUY5w7Q7yEtu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Short Answer Question:** Was cross-validation helpful in choosing the optimal max depth parameter? Why or why not? [2 pts]"
      ],
      "metadata": {
        "id": "a1ktoyWJwNRV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`[WRITE YOUR ANSWER HERE]`"
      ],
      "metadata": {
        "id": "H56gT-A7G0o7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 4: Compete! (5 pts, with up to 10 extra credit pts)\n",
        "Now, we can put our favorite models (or try additional variations!) to the test using the data that we haven't looked at in any part of the assignment (`houston_homes_test.csv`).\n",
        "\n",
        "Here, we will be using [Kaggle](https://www.kaggle.com/t/8ba216798d9f42c9bb4aa270dab061fd). If you do not already have a Kaggle account, you will need to make one.\n",
        "\n",
        "There are additional details on the Kaggle site, but some particularly important notes:\n",
        "* You are free to choose any team name (the name that will show up on the Kaggle leaderboard) as long as it is not inappropriate or offensive; however, in order to receive credit, you **must** specify your `team name` in your notebook here. If you do not, there is no way for us to assign you credit!\n",
        "* Kaggle lists the close date as several days after the homework's due date. This is because Kaggle does not support late submissions. The homework and your submission on Kaggle are due by the due date listed here, but you may use late days and turn it in late (i.e., if you submit Kaggle predictions after the due date, it will automatically count towards your late days even if you have turned in your notebook already).\n",
        "* This portion of the assignment **must** be completed independently. You cannot share prediction code or predictions with each other. In fact, you must put the exact code you use for your final predictions below. Violations will result in point deductions.\n",
        "* Related, you cannot modify your prediction files manually. Violations will result in point deductions.\n",
        "* You can only use regression models that we have discussed in class (though you can feel free to preprocess your data / tune any of the parameters in the models however you like)!\n",
        "\n",
        "For this homework, you will simply be graded for completion of a successful submission to Kaggle, and not on performance! But for fun, we have included several benchmarks on the Kaggle leaderboard, based on either simple heuristics or models.\n",
        "\n",
        "You can receive extra credit points for exceeding these benchmarks on the private leaderboard:\n",
        "* 1 pt for passing the `base-benchmark`\n",
        "* 1 pt for passing the `easy-benchmark`\n",
        "* 1 pt for passing the `medium-benchmark`\n",
        "* 1 pt for passing the `hard-benchmark`\n",
        "\n",
        "And additional points for doing well on the private leaderboard that will be revealed after the late due date (if there are ties, everyone tied will receive the same number of points):\n",
        "* 6 pts for 1st place\n",
        "* 4 pts for 2nd place\n",
        "* 2 pts for 3rd place\n"
      ],
      "metadata": {
        "id": "Dj2wlLBSa-Cd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Kaggle team name:** `[fill in here]`"
      ],
      "metadata": {
        "id": "gB6QSWqcL9fq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will finally read in the test dataset."
      ],
      "metadata": {
        "id": "5bn7vin7Q5Bj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = pd.read_csv(DATADIR + \"houston_homes_test.csv\")"
      ],
      "metadata": {
        "id": "WcLPKOr2Q4dS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: put all code needed (including preprocessing steps) to make your\n",
        "# final kaggle submission; note that this code must match the predictions\n",
        "# that you provide on kaggle\n"
      ],
      "metadata": {
        "id": "E3pLECLVRATU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see details about the file format for submission on kaggle (`sample_submission.csv`, essentially a 2 column file with `houseid`, the unique identifier in your test set, and `list_price`, your predictions). To make things easier, we provide here some sample code that you can modify to make your own submission file if your predictions were in a variable called `y_pred_kagg`."
      ],
      "metadata": {
        "id": "SgFllgUNRalw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = pd.Series(y_pred_kagg.flatten(), name=\"list_price\")\n",
        "results = pd.concat([df_test['houseid'], results], axis=1)\n",
        "results.to_csv('my_submission.csv', index=False)"
      ],
      "metadata": {
        "id": "2XeoeTNFwcdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you output your csv file, you need to download the file from colab to your local computer (you can click the file folder icon on the left panel to see the files in your workspace) and upload that file to the Kaggle site as your submission. Note that you can submit multiple times (up to 5 times a day)!"
      ],
      "metadata": {
        "id": "JFZ7sXqZXfWm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## To Submit\n",
        "Download the notebook from Colab as a `.ipynb` notebook (`File > Download > Download .ipynb`) and upload it to the corresponding Gradescope assignment. Your assignment should be named `netid-hw4.ipynb`.  \n",
        "\n",
        "Also, double check that your Kaggle submission shows up on the public leaderboard."
      ],
      "metadata": {
        "id": "VsysnFAd1zvg"
      }
    }
  ]
}