{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kreatorkat2004/kreatorkat2004/blob/main/comp341_hw6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## COMP 341: Practical Machine Learning\n",
        "## Homework Assignment 6: Wrapping it all up\n",
        "### Due: Tuesday, November 25 at 11:59pm on Gradescope"
      ],
      "metadata": {
        "id": "tDRlD1zm2Wyy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This assignment will first walk through some of the fundamentals of PyTorch as we build a deep learning classifier to identify items of clothing from black and white images, then revisit Homework 4, where we used regression-based methods to predict the list prices of homes in the Houston area. The dataset that we will be using in the first portion, [FashionMNIST](https://github.com/zalandoresearch/fashion-mnist), is often used for benchmarking and is inspired by the classic [MNIST](http://yann.lecun.com/exdb/mnist/) dataset of handwritten digits. For the second part, we will supplement our prediction task with also the first picture on the house listing."
      ],
      "metadata": {
        "id": "YiO0ySkS3nT0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As always, fill in missing code following `# TODO:` comments or `####### YOUR CODE HERE ########` blocks and be sure to answer the short answer questions marked with `[WRITE YOUR ANSWER HERE]` in the text.\n",
        "\n",
        "All code in this notebook will be run sequentially so make sure things work in order! Be sure to also use good coding practices (e.g., logical variable names, comments as needed, etc), and make plots that are clear and legible.\n",
        "\n",
        "For this assignment, there will be **15 points** allocated for general coding and formatting points:\n",
        "* **5 points** for coding style\n",
        "* **5 points** for code flow (accurate results when everything is run sequentially)\n",
        "* **5 points** for additional style guidelines listed below\n",
        "\n",
        "Additional style guidelines:\n",
        "* **The ipynb files are not rendering properly on gradescope due to size limits, so for the convenience of your TAs, please export a pdf of your colab notebook (and include a rice-accessible private link to the notebook at the end of the assignment). Your file should be named: `netid-hw6.pdf`**\n",
        "* For any TODO cell, make sure to include that cell's output in the .ipynb file that you submit. Many text editors have an option to clear cell outputs which is useful for getting a blank slate and running everything beginning-to-end, but always be sure to run the notebook before submitting and ensure that every cell has an output.\n",
        "* When displaying DataFrames, please do not include `.head()` or `.tail()` calls unless asked to. Just removing these calls will work as well, and will allow us to see both the beginning and end of your DataFrames, which help us ensure data is processed properly. Notebooks will by default show only the beginning and end, so you don't have to worry about long outputs here.\n",
        "* If column names are specified in the question, please use the specified name, and please avoid any sorting not specified in the instructions.\n",
        "* For plots, please ensure you have included axis labels, legends, and titles.\n",
        "* To format your short answer responses nicely, we recommend either **bolding** or *italicizing* your answer, or formatting it ```as a code block```.\n",
        "* Generally, please keep your notebook cells to one solution per cell, and preserve the order of the questions asked.\n",
        "* Finally, this can be harder to check/control and depends on which plotting libraries you prefer, but it would be helpful to limit the size/resolution of plot images in the notebook. Our grading platform has an upper limit on submission sizes it can display, and high-res plots are the usual culprit when submissions are hidden or truncated."
      ],
      "metadata": {
        "id": "jHU8BSdW3lrf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup\n",
        "First, we need to import the necessary libraries for this assignment."
      ],
      "metadata": {
        "id": "zMWKpgHsEs8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from PIL import Image, ImageOps\n",
        "import os\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "T4NSJ4H6E0uy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Packages such as [TorchMetrics](https://torchmetrics.readthedocs.io/en/stable/) provide more options for evaluation metrics than what PyTorch natively provides. Here, we install the package and load their implementation of MSE, since it natively supports RMSE (which we will use in Part 2 instead of implementing it from scratch)."
      ],
      "metadata": {
        "id": "Ju8-idiuDlGO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics"
      ],
      "metadata": {
        "id": "mJx7PDfMDkyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchmetrics import MeanSquaredError"
      ],
      "metadata": {
        "id": "yP1Zx9ikDst1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add additional modules/libraries to import here (rather than wherever you first use them)."
      ],
      "metadata": {
        "id": "7E7LwZ18E-BY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# additional modules / libraries to import\n"
      ],
      "metadata": {
        "id": "ARTBOl2dFG9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make sure we are using the latest torch version (PyTorch 2.8) and check if we have GPUs available (cu or cuda vs cpu)."
      ],
      "metadata": {
        "id": "ZQOkZtpxFjRg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.__version__"
      ],
      "metadata": {
        "id": "b31EITSHFy-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We would preferably like to also take advantage of any gpu resources to speed up our training. To this end, in Colab, you can click on the RAM / Disk monitoring bars towards the upper right corner of your Colab instance. Then in the menu that appears, click \"Change runtime type\" and select \"GPU\" under \"Hardware accelerator.\" Let's check if the GPU is indeed detected. Note that whis code snippet will by default assume we are using CPU resources unless it can detect GPUs available."
      ],
      "metadata": {
        "id": "NPTXzf0X5DWD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cpu\"\n",
        "if (torch.cuda.is_available()):\n",
        "  device = \"cuda\"\n",
        "print(\"device: \" + device)"
      ],
      "metadata": {
        "id": "eV_iGHK_5LmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `device` string simply will be used in a few key parts of the code to signal to PyTorch whether or not to move the model / weights etc to the GPU, if available. Using GPUs for this assignment is optional (though doing so will allow you to train faster)."
      ],
      "metadata": {
        "id": "G0lwd-pn_H_O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As in previous assignments, we will use Google Drive to access the dataset that you will use in the second half of the homework. However, there are some slight modifications once more because your `DataLoader` will be frequently accessing the raw image files, which can be very slow if it is having to access the files on Google Drive constantly.\n",
        "\n",
        "By now, you will probably already created your local `comp341` directory:\n",
        "1. Go to 'My Drive' in your own Google Drive\n",
        "2. Make a new folder named `comp341`\n",
        "\n",
        "Now, we will copy a shortcut to a zipped data file with all of the images and metadata for the houses:\n",
        "\n",
        "3. From the [Google Drive link](https://drive.google.com/file/d/1H3QvduNrpfEIn6Q4ODEc6gpnr9eiysLJ/view?usp=sharing), you can right click the `comp341-hw6.zip` file, and select `Add shortcut to Drive`, and add a link to the file to your `comp341` folder. This is a convenient alternative to having to download and re-upload the files to your own drive.\n",
        "\n",
        "If you run into trouble with accessing the files from the shortcut, then:\n",
        "\n",
        "4. Download the `comp341-hw6.zip` file and click `New -> File Upload` to upload the downloaded file from your computer.\n",
        "\n",
        "Now, we will mount your Google Drive as usual, but we will have an additional step of copying over the zip file to your local colab instance so that files can be accessed quickly."
      ],
      "metadata": {
        "id": "p9RTNP_eK7cP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# note that this command will trigger a request from google to allow colab\n",
        "# to access your files: you will need to accept the terms in order to access\n",
        "# the files this way\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# if you followed the instructions above exactly, your zipped data file should\n",
        "# be located at the file path below; if your files are in a different directory\n",
        "# on your Google Drive, you will need to change the path below accordingly\n",
        "ZIPPATH = '/content/drive/My Drive/comp341/comp341-hw6.zip'"
      ],
      "metadata": {
        "id": "iQRjNIFFMybe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp \"{ZIPPATH}\" .\n",
        "!unzip -q \"comp341-hw6.zip\"\n",
        "!rm \"comp341-hw6.zip\""
      ],
      "metadata": {
        "id": "soDMHE0OM5Aj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In your local colab instance, you should now have a `house_imgs/` directory with many images of homes (includes images from both the training and test sets), as well as two csv files: `home_data_train.csv` and `home_data_test.csv`."
      ],
      "metadata": {
        "id": "xT0UrHP-M_8f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 1: Simple CNN using FashionMNIST [28 pts]\n",
        "\n",
        "PyTorch provides convenient functionality to download and load Fashion-MNIST as a `Dataset` that can be used with `DataLoader`!"
      ],
      "metadata": {
        "id": "-S3Jvj-HDyOW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BC3Wretu2S1x"
      },
      "outputs": [],
      "source": [
        "# we can load the data directly using PyTorch\n",
        "# this will automatically download the data into your colab folder (and redownload as necessary)\n",
        "fm_data_train = torchvision.datasets.FashionMNIST('./data', download=True,\n",
        "                                                  transform=transforms.Compose([transforms.ToTensor()]))\n",
        "fm_data_valid = torchvision.datasets.FashionMNIST('./data', download=True, train=False,\n",
        "                                                  transform=transforms.Compose([transforms.ToTensor()]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# based on the label definitions given by the original dataset\n",
        "# https://github.com/zalandoresearch/fashion-mnist#labels\n",
        "# we have provided a convenient function to get the actual labels\n",
        "# corresponding to each numeric label in the dataset\n",
        "def label_name(label):\n",
        "  label_mapping = {\n",
        "      0: \"T-shirt/Top\",\n",
        "      1: \"Trouser\",\n",
        "      2: \"Pullover\",\n",
        "      3: \"Dress\",\n",
        "      4: \"Coat\",\n",
        "      5: \"Sandal\",\n",
        "      6: \"Shirt\",\n",
        "      7: \"Sneaker\",\n",
        "      8: \"Bag\",\n",
        "      9: \"Ankle Boot\"\n",
        "      }\n",
        "  label_num = (label.item() if type(label) == torch.Tensor else label)\n",
        "  return label_mapping[label_num]"
      ],
      "metadata": {
        "id": "wDBLSzvBmm4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we can now loop through a couple of the first few images in our dataset\n",
        "# note that we enlarge the image simply for easy visualization purposes\n",
        "\n",
        "tens_to_img = transforms.ToPILImage()\n",
        "for i in range(4):\n",
        "  print('label id:', fm_data_train[i][1], 'name:', label_name(fm_data_train[i][1]))\n",
        "  display(tens_to_img(fm_data_train[i][0]).resize((150,150)))"
      ],
      "metadata": {
        "id": "_OhEOuhfHJfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: how many observations are there in your training and validation sets? [1 pt]\n"
      ],
      "metadata": {
        "id": "RtOVtX8qqmss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: make data loaders for the training and validation datasets\n",
        "# that we loaded above; for now, use a batch size that will allow you to\n",
        "# use Stochastic Gradient Descent in the training [2 pts]\n"
      ],
      "metadata": {
        "id": "ee1gCHl4HU_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the data description, we know that there are 10 labels (with mappings between number and actual category provided by our `label_name` method). Let's use the DataLoaders that we just made to count up the number of observations we have per label."
      ],
      "metadata": {
        "id": "X6tKSbDjrQVq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: loop through the entire dataset using the DataLoaders you constructed earlier\n",
        "# and count how many observations we have per label (in both training and validation sets)\n",
        "# show the results using the label name (not simply the number representation) [1 pts]\n"
      ],
      "metadata": {
        "id": "XazeV0EnKiXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now construct our model using CNNs with a multilayer perceptron at the end for classification. As the data description shows (and we could also verify easily), each image is grayscale, 28 x 28 pixels.\n",
        "\n",
        "The TODOs below describe the details of the architecture. Implementaiton-wise, you are free to declare the full architecture in `__init__` or a combination of `__init__` and `forward`. As we discussed in class, there are multiple ways to implement the same architecture. Feel free to opt for the one you feel is most intuitive."
      ],
      "metadata": {
        "id": "ldhpDqC9J-JT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first model attempt\n",
        "class StylishNN(nn.Module):\n",
        "  def __init__(self, num_classes=10):\n",
        "    # TODO: inherit from nn.Module [1 pt]\n",
        "\n",
        "\n",
        "    # TODO: add a first convolution layer together with a ReLU activation function\n",
        "    # for this first convolution layer, we want to apply 12 filters, where each\n",
        "    # filter is 5x5, taking strides of 1, and padded to maintain the original image size\n",
        "    # follow up on this convolution with max pooling with a 2x2 filter and stride 2 [2 pts]\n",
        "\n",
        "\n",
        "\n",
        "    # TODO: add a second convolution layer that expands the number of filters to 32\n",
        "    # the filters will still be 5x5 and taking strides of 1, but this time, use a 1 pixel padding instead\n",
        "    # follow up on this convolution with max pooling again with a 2x2 filter and stride 2 and a ReLU activation function [2 pts]\n",
        "\n",
        "\n",
        "\n",
        "    # TODO: for the classification portion of this neural network, make 3 fully connected layers\n",
        "    # you will need to calculate what the current dimensions of your tensors are after convolution\n",
        "    # and pooling, as you the fully connected layers will use the flattened values\n",
        "    # with that number as your initial input, here are the number of neurons for each layer, together\n",
        "    # with which activation function we would like to use [3 pts]\n",
        "    # fully connected layer 1: 600 neurons, ReLU activation\n",
        "    # fully connected layer 2: 120 neurons, ReLU activation\n",
        "    # fully connected layer 3 (output layer): num_classes neurons, no additional activation function,\n",
        "    # use the log(softmax) of these values to correspond to a probability for each class\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    # TODO: forward pass for the convolution and pooling layers [1 pt]\n",
        "\n",
        "\n",
        "    # TODO: flatten parameters into 1 dimension before passing them\n",
        "    # on to our fully connected layers [1 pt]\n",
        "\n",
        "\n",
        "    # TODO: forward pass for the fully connected layers [1 pt]\n",
        "\n",
        "\n",
        "    # TODO: return the log(softmax) predictions for each class [1 pt]\n",
        "\n"
      ],
      "metadata": {
        "id": "QsTf_h3vKQoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've constructed our model, it is time to train. Instead of stochastic gradient descent, we would prefer to use mini-batch gradient descent. For the next parts, set up your `DataLoader`s to use batch sizes of 64."
      ],
      "metadata": {
        "id": "PirbsRwOSTMt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: make data loaders for the training and validation datasets\n",
        "# with a batch size of 64; remember that we want to randomize the order that we see our data! [1 pts]\n"
      ],
      "metadata": {
        "id": "lw60Fu3z6AFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: fill in the steps that we want to take within the training loop\n",
        "# (this function will run once per epoch) [3 pts]\n",
        "# we have provided some helper code for logging progress\n",
        "\n",
        "def train(model, train_loader, opt, epoch, verbose=False):\n",
        "  if verbose:\n",
        "    print(\"starting epoch\", epoch)\n",
        "  train_loss = 0\n",
        "  for i, (image, label) in enumerate(train_loader):\n",
        "    image, label = image.to(device), label.to(device)\n",
        "\n",
        "    # TODO: a) forward pass\n",
        "\n",
        "\n",
        "    # TODO: b) calculate loss\n",
        "\n",
        "\n",
        "    # TODO: c) backward pass\n",
        "\n",
        "\n",
        "    # TODO: d) update weight estimates\n",
        "\n",
        "\n",
        "    # TODO: e) reset gradients to zero\n",
        "\n",
        "\n",
        "    # we are tracking the sum of losses to calculate the average training loss for this epoch\n",
        "    train_loss += loss.item()\n",
        "\n",
        "    # when verbose is on, we will show how the loss is changing across batches within one epoch\n",
        "    if verbose and ((i % 100) == 0):\n",
        "      print('training [epoch {}: {}/{} ({:.0f}%)] loss: {:.6f}'.format(\n",
        "          epoch, i * len(image), len(train_loader.dataset),\n",
        "          100. * i / len(train_loader), loss.item()))\n",
        "\n",
        "  avg_tl = train_loss / (i+1)\n",
        "  print('epoch {} avg training loss: {:.6f}'.format(epoch, avg_tl))\n",
        "  return avg_tl"
      ],
      "metadata": {
        "id": "JLmSCg-pq9u1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we have provided the below code for reporting loss and accuracy\n",
        "# on your validation set, so simply run this snippet without any changes\n",
        "def valid(model, valid_loader):\n",
        "  valid_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for i, (image, label) in enumerate(valid_loader):\n",
        "      image, label = image.to(device), label.to(device)\n",
        "\n",
        "      # get the prediction\n",
        "      pred = model(image)\n",
        "\n",
        "      # get loss\n",
        "      valid_loss += loss_fn(pred, label).item()\n",
        "\n",
        "      # calculate the accuracy for this batch\n",
        "      _, pred = torch.max(pred.data, 1)\n",
        "      correct += torch.sum(label==pred).item()\n",
        "\n",
        "  # get the loss for the epoch\n",
        "  avg_vl = valid_loss / (i+1)\n",
        "  print('avg validation loss: {:.6f}, accuracy: {}/{} ({:.0f}%)'.format(\n",
        "      avg_vl, correct, len(valid_loader.dataset),\n",
        "      100. * correct / len(valid_loader.dataset)))\n",
        "\n",
        "  return avg_vl"
      ],
      "metadata": {
        "id": "L-M2zn03e27h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we can now initialize the model you designed above\n",
        "# and optionally put it on the GPU if we have enabled it\n",
        "# remember to rerun this if you want to re-initialize your parameters!\n",
        "# (otherwise, the model will simply keep updating the previous parameters)\n",
        "model = StylishNN().to(device)\n",
        "\n",
        "# TODO: initialize the optimizer with learning rate 0.01 [1 pt]\n",
        "\n",
        "\n",
        "# TODO: initialize cross entropy as your loss function [1 pt]\n",
        "\n"
      ],
      "metadata": {
        "id": "sb_IRmkitaaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now that you have defined your model and set up the training loop\n",
        "# we can run through 15 epochs and see how our training and validation\n",
        "# losses change over time (you can just run this code directly)\n",
        "# note that we have turned on verbose here to better see intermediate progress\n",
        "# but if the output is overwhelming, you can feel free to turn it off\n",
        "epoch_list = []\n",
        "train_loss = []\n",
        "valid_loss = []\n",
        "\n",
        "epochs = 15\n",
        "for e in range(1, epochs+1):\n",
        "  epoch_list.append(e)\n",
        "  train_loss.append(train(model, train_loader, opt, e, verbose=True))\n",
        "  valid_loss.append(valid(model, valid_loader))"
      ],
      "metadata": {
        "id": "bBCTMVtyvRlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will plot the training and test over the different epochs as well as look for any confusion between classes."
      ],
      "metadata": {
        "id": "B94j1t1t0UVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: as we trained our model, we captured the epoch,\n",
        "# average loss for the training and validation sets in\n",
        "# epoch_list, train_loss, and valid_loss\n",
        "# use these to plot the loss vs epochs for the training and test set [2 pts]\n",
        "\n"
      ],
      "metadata": {
        "id": "eyZ7mqFJ0YMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Short Answer Question:** What does this epoch vs loss plot tell you about your model and predictions? [1 pt]"
      ],
      "metadata": {
        "id": "FVn8sdNwFn-M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`[WRITE YOUR ANSWER HERE]`"
      ],
      "metadata": {
        "id": "DZyDwFfJFuyY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: use sklearn's convenient confusionmatrix and ConfusionMatrixDisplay\n",
        "# methods to calculate and visualize the confusion matrix for your\n",
        "# current predictions, making sure to display the human-readable\n",
        "# class labels (instead of 0-9) [2 pts]\n",
        "\n"
      ],
      "metadata": {
        "id": "PWBE2j8m3asb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Short Answer Question:** Does our model have any trouble distinguishing any items of clothing? If so, which ones does it tend to mixed up? [1 pt]\n"
      ],
      "metadata": {
        "id": "VsFs3Ubf8glF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`[WRITE YOUR ANSWER HERE]`"
      ],
      "metadata": {
        "id": "FxlFlJE-8lPO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 2: List Price Revisited [27 pts]\n",
        "\n",
        "In the ```Setup``` section, you should have copied over the house images and tabular data into your colab instance.\n",
        "\n",
        "Since we explored this task in Homework 4, we are already somewhat familiar with the tabular features, including some of the data cleaning / preprocessing steps that might be valuable, as well as which ones may be more or less valuable to keep.\n",
        "\n",
        "Now that we also have some sense of what the house listing images are like based on Part 1, we will set up a regression framework that can use both the tabular features and image features. Along the way, we will also see how our predictions may change depending on what data we use."
      ],
      "metadata": {
        "id": "VLkDbFOkNuwl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We provide several helper functions for setting up the data and functionality to visualize individual examples, which can sometimes be helpful to get a sense of what the model is doing."
      ],
      "metadata": {
        "id": "_N698K-zC1zO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# torch converts the 0-255 RGB values to 0-1 tensors, but it can\n",
        "# also be beneficial to also standardize the values (or, as we\n",
        "# see here, subtract the mean RGB values from the images)\n",
        "\n",
        "# these transformations below help facilitate this\n",
        "# inv_normalize is provided mainly for visualization sake, so that\n",
        "# we can flip the standardization process to see the image in its\n",
        "# original colors\n",
        "\n",
        "house_mean = [0.5230, 0.5416, 0.4989]\n",
        "# house_sd = [0.2271, 0.2162, 0.2640]\n",
        "# only subtracting mean and not also dividing by standard deviation\n",
        "# can actually sometimes work better, which is what we are doing here\n",
        "house_sd = [1, 1, 1]\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(house_mean, house_sd)\n",
        "])\n",
        "\n",
        "inv_normalize = transforms.Normalize(\n",
        "   mean= [-m/s for m, s in zip(house_mean, house_sd)],\n",
        "   std= [1/s for s in house_sd]\n",
        ")\n"
      ],
      "metadata": {
        "id": "cslAMjA3CxNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convenient function for displaying images\n",
        "# by default, will reverse the standardization calculation so that we can\n",
        "# see the images in a \"normal\" color scheme\n",
        "def display_data(d, inv_norm=True):\n",
        "  if isinstance(d['houseid'], list): # we can handle a list of houses\n",
        "    batch_size = len(d['houseid'])\n",
        "    for i in range(batch_size):\n",
        "      if 'price' in d:\n",
        "        print('price:', \"${:,.0f}\".format(d['price'][i]))\n",
        "\n",
        "      if inv_norm:\n",
        "       display(transforms.ToPILImage()(inv_normalize(d['image'][i])))\n",
        "      else:\n",
        "        display(transforms.ToPILImage()(d['image'][i]))\n",
        "  else: # only an individual house to be displayed\n",
        "    if 'price' in d:\n",
        "      print('price:', \"${:,.0f}\".format(d['price']))\n",
        "\n",
        "    if inv_norm:\n",
        "      display(transforms.ToPILImage()(inv_normalize(d['image'])))\n",
        "    else:\n",
        "      display(transforms.ToPILImage()(d['image']))"
      ],
      "metadata": {
        "id": "RVO8eyvsC4cY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you may recall, data handling in PyTorch relies heavily on the `Dataset` class. In Part 1, we used one of the pre-built `Datasets`, but most of the time, we need to set up our own. Here, we provide a scaffold `HouseImagesDataset` class.\n",
        "\n",
        "You will need to fill in any cleaning / data preprocessing steps for the features in `__init__`. We provide code for `__getitem__`, but you will also need to fill in the other method we typically define for a custom `Dataset`: `__len__`."
      ],
      "metadata": {
        "id": "IuR4ykWOC7sA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HouseImagesDataset(Dataset): # [4 pts]\n",
        "    def __init__(self, annot_file, image_dir, train=True):\n",
        "        # the annotation file is tidy, aka each row is a unique observation in the dataset,\n",
        "        # but it is not yet clean, which you will address in the TODO below\n",
        "        df = pd.read_csv(annot_file)\n",
        "\n",
        "        # TODO: cleaning / preprocessing of features in df\n",
        "\n",
        "\n",
        "\n",
        "        # TODO: fill in this feature_cols list with the column names of\n",
        "        # features you would like to use to predict list price (many of the columns\n",
        "        # will likely be transformed from the original data in annot_file)\n",
        "        self.feature_cols = []\n",
        "\n",
        "        self.house_annot = df\n",
        "        self.image_dir = image_dir\n",
        "        self.train=train\n",
        "\n",
        "    def __len__(self):\n",
        "        # TODO: fill in this method (replacing pass) to return the length of the dataset\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        # we have provided code that will load and transform the indexed (\"ith\") image\n",
        "        # as well as features specified earlier in self.feature_cols within the processed\n",
        "        # pandas DataFrame\n",
        "        img = Image.open(os.path.join(self.image_dir, self.house_annot.loc[idx, 'houseid'] + '.jpg'))\n",
        "        img = transform(img)\n",
        "\n",
        "        features = self.house_annot.loc[idx, self.feature_cols]\n",
        "        features = features.tolist()\n",
        "        features = torch.as_tensor(features, dtype=torch.float)\n",
        "\n",
        "        # depending on whether the Dataset is in training mode, we will have the price data or not\n",
        "        if self.train:\n",
        "            item = {'image': img,\n",
        "                    'houseid': self.house_annot.loc[idx, 'houseid'],\n",
        "                    'features': features,\n",
        "                    'price': torch.as_tensor(self.house_annot.loc[idx, 'list_price'], dtype=torch.float)}\n",
        "        else:\n",
        "            item = {'image': img,\n",
        "                    'houseid': self.house_annot.loc[idx, 'houseid'],\n",
        "                    'features': features}\n",
        "\n",
        "        return item"
      ],
      "metadata": {
        "id": "IE8wyjzNDADf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: initialize the house dataset using the training data you were provided and check the length of the dataset [1 pt]\n"
      ],
      "metadata": {
        "id": "UG9X-brBDDaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: check that the data loads properly by calling the provided display_data function with a specifically indexed\n",
        "# item in your house dataset (e.g., house_dataset[3]) [1 pt]\n"
      ],
      "metadata": {
        "id": "U-kIGagXDE0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: use the convenient torch.utils.data.random_split function to split your loaded dataset into training and\n",
        "# validation portions, using 75% of the data for training and 25% of the data for validation [1 pt]\n"
      ],
      "metadata": {
        "id": "wlwu7M-RDMl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We explored how to set up a CNN with several convolutional layers and some fully connected layers in Part 1 when we have image data. Now, we have *both* image and tabular features. One natural way we can handle this is to set up several convolutional layers to extract important image-related features, and in parallel, pass our tabular features that we indicated we want to use (`self.feature_cols` in our `HouseImagesDataset`) through a simple MLP, then simply **concatenate** the two sets of features together before passing them through a set of linear layers before reaching our final prediction.\n",
        "\n",
        "Recall that with house price predictions, we want to end up with a single non-zero prediction for each house."
      ],
      "metadata": {
        "id": "Va1ZVBjvDPfx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HybridHouseNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # TODO: set up some convolutional layers\n",
        "    # it is your choice as to how many convolutional blocks, as well as\n",
        "    # specifics within the blocks: activation function, pooling, etc [1 pt]\n",
        "\n",
        "\n",
        "    # TODO: set up an MLP for the tabular features that you will be\n",
        "    # inputting into this model [1 pt]\n",
        "\n",
        "\n",
        "    # TODO: set up the final set of fully connected layers that\n",
        "    # takes as input the concatenated set of flattened convolution features\n",
        "    # together with the output of the MLP from the tabular features\n",
        "    # to eventually output a single non-negative prediction [1 pt]\n",
        "\n",
        "\n",
        "  def forward(self, ximg, xfeats):\n",
        "    # TODO: write out the forward pass steps\n",
        "    # note that forward now has 2 inputs because we are using both\n",
        "    # images and non-image features separately at first, before\n",
        "    # merging them together for the final set of predictions\n",
        "    # Note: you may also need to adjust the shape of your final prediction\n",
        "    # so that it plays nice with the loss function etc. [2 pts]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XyjXkuabDPPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before training our model, we want to also set up some additional models to what the differences might be if we use *only* images or *only* the tabular features for our predictions. Of course, if we set the models up differently with different hyperparameters, we really cannot have a truly equivalent comparison, but we will try to keep as many of the model blocks the same as possible."
      ],
      "metadata": {
        "id": "CjwR2raiDWTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HouseImageOnly(nn.Module):\n",
        "  def __init__(self):\n",
        "    # TODO: set up this model to only use images for predictions,\n",
        "    # using the same convolutional blocks that you designed for\n",
        "    # your HybridHouseNN model; also use a similar set of\n",
        "    # fully connected layers (changing only the input dimension\n",
        "    # since you will no longer be concantenating other features) [2 pts]\n",
        "\n",
        "\n",
        "  def forward(self, ximg):\n",
        "    # TODO: the forward pass will now only have the image as input [1 pt]\n"
      ],
      "metadata": {
        "id": "EP8Df7RNDX6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HouseFeatsOnly(nn.Module):\n",
        "  def __init__(self):\n",
        "    # TODO: set up this model to only use the tabular, non-image features\n",
        "    # this model will simply be a MLP with those features as input\n",
        "    # once again, try to keep the layers relatively comparable to what\n",
        "    # you chose in HybridHouseNN, though of course the dimensionality going\n",
        "    # into the last set of fully connected layers will likely change drastically\n",
        "    # without image features [2 pts]\n",
        "\n",
        "\n",
        "  def forward(self, xfeats):\n",
        "    # TODO: the forward pass will now only have non-image features as input [1 pt]\n",
        "\n"
      ],
      "metadata": {
        "id": "NrzGPJMrDZhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As noted in the Setup section, will use RMSE for our loss function:"
      ],
      "metadata": {
        "id": "Cyk7IOgBDcCp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = MeanSquaredError(squared=False).to(device)"
      ],
      "metadata": {
        "id": "PRbjN9okDeBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's fill in the details of the training and validation methods."
      ],
      "metadata": {
        "id": "vJk5ngE9DgVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, opt, epoch, mode=\"both\", verbose=False):\n",
        "  # mode can be \"both\", \"image\", or \"features\", depending on if we are using\n",
        "  # our HybridHouseNN, HouseImageOnly, or HouseFeatsOnly model\n",
        "  # we will assume that the model passed to this function matches the mode,\n",
        "  # and mode will affect whether the model uses image, features, or a combination\n",
        "  # as input to get the predictions in the forward pass\n",
        "\n",
        "  if verbose:\n",
        "    print(\"starting epoch\", epoch)\n",
        "  train_loss = 0\n",
        "  for i, batch in enumerate(train_loader):\n",
        "    image, features, price = batch['image'].to(device), \\\n",
        "                             batch['features'].to(device), \\\n",
        "                             batch['price'].to(device)\n",
        "\n",
        "    model.train(True)\n",
        "\n",
        "    # TODO: fill in the code for each of the steps in the\n",
        "    # training loop, remembering that we want to account for\n",
        "    # different modes in the forward pass step [2 pts]\n",
        "\n",
        "\n",
        "    model.train(False)\n",
        "    if verbose and ((i % 20) == 0):\n",
        "      print('training [epoch {}: {}/{} ({:.0f}%)] loss: {:.6f}'.format(\n",
        "          epoch, i * len(image), len(train_loader.dataset),\n",
        "          100. * i / len(train_loader), loss.item()))\n",
        "\n",
        "  avg_tl = train_loss / (i+1)\n",
        "  print('epoch {} avg training loss: {:.6f}'.format(epoch, avg_tl))\n",
        "  return avg_tl"
      ],
      "metadata": {
        "id": "OQVoJipaDf0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def valid(model, valid_loader, mode=\"both\"):\n",
        "  # as in train, mode can be \"both\", \"image\", or \"features\", depending on if we are using\n",
        "  # our HybridHouseNN, HouseImageOnly, or HouseFeatsOnly model\n",
        "  # we will assume that the model passed to this function matches the mode,\n",
        "  # and mode will affect whether the model uses image, features, or a combination\n",
        "  # as input to get the predictions in the forward pass\n",
        "  valid_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for i, batch in enumerate(valid_loader):\n",
        "      image, features, price = batch['image'].to(device), \\\n",
        "                               batch['features'].to(device), \\\n",
        "                               batch['price'].to(device)\n",
        "\n",
        "      # TODO: fill in code to calculate pred (the prediction), paying attention to\n",
        "      # different usage of the model depending on the inputted mode variable [1 pt]\n",
        "\n",
        "\n",
        "      # get loss\n",
        "      valid_loss += loss_fn(pred, price).item()\n",
        "\n",
        "\n",
        "  # get the loss for the epoch\n",
        "  avg_vl = valid_loss / (i+1)\n",
        "  print('avg validation loss: {:.6f}'.format(avg_vl))\n",
        "\n",
        "  return avg_vl"
      ],
      "metadata": {
        "id": "YhzM9gYhD3Fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: initialize your training and validation DataLoaders, using a batch_size of 64\n",
        "# and remembering to randomize the order the data points are presented to the model [1 pt]\n",
        "\n"
      ],
      "metadata": {
        "id": "HaVa036-D4k2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we will use these simple dictionaries to keep track of the loss for our 3 different models\n",
        "# note that depending on how complex your 3 models are, training may take some time, on the scale\n",
        "# of 10-30 minutes\n",
        "# with verbose on below, you should be seeing continual progress during training: if not, then\n",
        "# double check that you are using GPUs and the image files locally within your colab instance\n",
        "epoch_list = defaultdict(list)\n",
        "train_loss = defaultdict(list)\n",
        "valid_loss = defaultdict(list)\n",
        "\n",
        "epochs = 30\n",
        "modes = {'both': HybridHouseNN(), 'image': HouseImageOnly(), 'features': HouseFeatsOnly()}\n",
        "\n",
        "for m in modes:\n",
        "  model = modes[m].to(device)\n",
        "  # TODO: initialize the optimizer (and associated hyperparameters like learning rate) of your choice [1 pt]\n",
        "  opt = optim.#OPTIMIZER\n",
        "\n",
        "  print(\"Current mode:\", m)\n",
        "  for e in range(1, epochs+1):\n",
        "    epoch_list[m].append(e)\n",
        "    train_loss[m].append(train(model, train_loader, opt, e, m, verbose=True))\n",
        "    valid_loss[m].append(valid(model, valid_loader, m))"
      ],
      "metadata": {
        "id": "Ddlp7rygD5mS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: on a single plot, plot the training and validation losses for your 3 different models [2 pts]\n"
      ],
      "metadata": {
        "id": "RMOIIkH8EFnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Short Answer Question:** Based on the results that you see in your plot, what do you think about using images, tabular features, or both for predicting list price? (Of course, the results and associated interpretations may change depending on how the models are set up, so describe your interpretations solely based on the plot that you generated above and not other potential possibilities.) [2 pts]"
      ],
      "metadata": {
        "id": "xhFArXqmEN6e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`[WRITE YOUR ANSWER HERE]`"
      ],
      "metadata": {
        "id": "a8Sh_jcuEQlE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 3: Optimizing Performance [30 pts]\n",
        "As before, we will be using [Kaggle](https://www.kaggle.com/t/293ccc99ce8b4b638c26fb46800a762e) with held out data to test the performance of our model.\n",
        "\n",
        "For this homework, *you will be graded on your ability to pass perfomance benchmarks*. To receive full credit for this section of the assignment you will need to pass 3 benchmarks (baseline, easy, moderate) ***on both the public and private leaderboard***. Remember that since the private leaderboard is a part of your grade, you want to refrain from solely overfitting to the public leaderboard! Partial credit will be allotted depending on how many benchmarks are passed.\n",
        "\n",
        "\n",
        "The top three leaders on the private leaderboard will recieve extra credit (if there are ties, everyone tied will receive the same number of points):\n",
        "* 5 points for first place\n",
        "* 3 points for second place\n",
        "* 2 points for third place\n",
        "\n",
        "\n",
        "The following Kaggle notes from the previous assignments still apply:\n",
        "* You can use any team name (the name that will show up on the Kaggle leaderboard) as long as it is not inappropriate or offensive; however, in order to receive credit, you **must** specify your `team name` in your notebook here. If you do not, there is no way for us to assign you credit!\n",
        "* Kaggle lists the close date as several days after the homework's due date. This is because Kaggle does not support late submissions. The homework and your submission on Kaggle are due by the due date listed here, but you may use late days and turn it in late (i.e., if you submit Kaggle predictions after the due date, it will automatically count towards your late days even if you have turned in your notebook already).\n",
        "* This portion of the assignment **must** be completed independently. You cannot share prediction code or predictions with each other. In fact, you must put the exact code you use for your final predictions below. Violations will result in point deductions.\n",
        "* Related, you cannot modify your prediction files manually. Violations will result in point deductions.\n",
        "* **Unlike previous assignments**, you are free to use *any* classification models you would like for this kaggle competition!"
      ],
      "metadata": {
        "id": "-8hLBaxl0JZB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Some helpful tips:\n",
        "\n",
        "* You are not required to use all of training data for this challenge: strategically applying filters and/or transformations (on observations or features, or both) can be helpful\n",
        "* Remember that you are free to use *any* regression model you would like, regardless of whether we discussed it in class---the world is your oyster!\n",
        "* As always, you want to check if your model is robust to different validation sets so that you are not overfitting to the public leaderboard."
      ],
      "metadata": {
        "id": "bTk9pA1N0ali"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Kaggle team name:** `[fill in here]`"
      ],
      "metadata": {
        "id": "w5_x_NLa0faB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read in the test dataset (`home_data_test.csv` and load associated images if you are using them)."
      ],
      "metadata": {
        "id": "16F7x1nK0hxR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: put all code needed (including preprocessing steps) to make your\n",
        "# final kaggle submission; note that this code must match the predictions\n",
        "# that you provide on kaggle\n"
      ],
      "metadata": {
        "id": "zVQubkJc0kwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see details about the file format for submission on kaggle (`sample_submission.csv`, essentially a 2 column file with `houseid`, the unique identifier in your test set, and `price`, your predictions). To make things easier, we provide here some sample code that you can modify to make your own submission file if your predictions were in a variable called `y_pred_kagg`. On Kaggle, the performance metric is RMSE as it was in Part 2 of this assignment."
      ],
      "metadata": {
        "id": "yFN9ybkqIYZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = pd.Series(y_pred_kagg.flatten(), name=\"price\")\n",
        "results = pd.concat([df_test['houseid'], results], axis=1)\n",
        "results.to_csv('my_submission.csv', index=False)"
      ],
      "metadata": {
        "id": "Pwu7Br9JIbzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you output your csv file, you need to download the file from colab to your local computer (you can click the file folder icon on the left panel to see the files in your workspace) and upload that file to the Kaggle site as your submission. Note that you can submit multiple times (up to 10 times a day)!"
      ],
      "metadata": {
        "id": "fP4PzCBTIdm7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## To Submit\n",
        "Please provide a Google Colab link (enable `Viewer` permissions to `Rice University` only) by clicking the `Share` button and toggling permissions accordingly and pasting the link here (in markdown, within the parantheses): [Colab notebook](https://)\n",
        "\n",
        "Now export the notebook as a PDF (`File > Print > Save as PDF`)."
      ],
      "metadata": {
        "id": "KulRL_mWdqLk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make sure your pdf reflects your full notebook. Google Colab can sometimes export the notebook strangely (see [GitHub Issue](https://github.com/googlecolab/colabtools/issues/4357)).\n",
        "\n",
        "If you run into substantial issues, it may be easier to convert your notebook to an html file and print directly from your browser. With your Google Drive already mounted, first you need to navigate to the folder with the file:"
      ],
      "metadata": {
        "id": "ITx_txQ6BtHd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this file path assumes the default location colab notebooks are stored on drive\n",
        "# if you use folders or put them in other directories, you will need to adjust the file path\n",
        "%cd /content/drive/MyDrive/Colab\\ Notebooks/\n",
        "!ls"
      ],
      "metadata": {
        "id": "kP4-WOrJBup4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the ls command should have shown you your notebook, you can now replace the name of the\n",
        "# notebook file in the below command\n",
        "!jupyter nbconvert --to html netid-hw6.ipynb\n",
        "\n",
        "# after this commmand, you should have a netid-hw6.html file in your Google Drive\n",
        "# you can now download that file, open it in your browser and directly save as pdf"
      ],
      "metadata": {
        "id": "IpzRp_9WBvPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make sure your pdf file is named `netid-hw6.pdf`, and upload it to the corresponding Gradescope assignment.\n",
        "\n",
        "Also, double check that your Kaggle submission shows up on the public leaderboard."
      ],
      "metadata": {
        "id": "uRvTWP9mBxDy"
      }
    }
  ]
}