{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kreatorkat2004/kreatorkat2004/blob/main/comp341_hw2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## COMP 341: Practical Machine Learning\n",
        "## Homework assignment 2: movie reviews\n",
        "\n",
        "### Due: Tuesday, September 23 at 11:59pm on Gradescope"
      ],
      "metadata": {
        "id": "nvy8zi1OFzcK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this homework, we will be looking at movie reviews scraped from [Rotten Tomatoes](https://www.rottentomatoes.com). Rotten Tomatoes is a website that contains reviews for both movies and TV shows. Professional movie critics and any user on the site (audience) can write reviews and give movies a score from 0 (rotten / spilled) to 100 (fresh / upright). Often times, the scores for a particular movie will diverge between critics and the audience, especially for polarizing movies or \"cult classics.\" For this assignment, let's assume we are working for a movie production company that wants to find a critic for evaulating the viability of movie screenplays. In this scenario, we are tasked with finding a critic or critics that are in touch with what the general public likes and thus are a good predictor of how the audience will rate a movie. Our production company is also interested in broadly how movie genre may play a role in all of this.\n",
        "\n",
        "While the scraped data is reasonably well structured, movie critics do not rate movies in a standardized way. This will be a good challenge for our data cleaning skills.\n",
        "\n",
        "We will have to leverage more advanced data reshaping techniques. While basically everything can be implemented by variations of the functionalities we've covered in class, pandas also provides [useful guides for reshaping](https://pandas.pydata.org/pandas-docs/stable/user_guide/reshaping.html), with convenient functions that may be helpful in this assignment."
      ],
      "metadata": {
        "id": "ItbdFZFDF7Hq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As always, fill in missing code following `# TODO:` comments or `####### YOUR CODE HERE ########` blocks and be sure to answer the short answer questions marked with `[WRITE YOUR ANSWER HERE]` in the text.\n",
        "\n",
        "All code in this notebook will be run sequentially so make sure things work in order! Be sure to also use good coding practices (e.g., logical variable names, comments as needed, etc), and make plots that are clear, legible, and easily interpretable (including the ones evaluating performance metrics).\n",
        "\n",
        "For this assignment, there will be **15 points** allocated for general coding and formatting points:\n",
        "* **5 points** for coding style\n",
        "* **5 points** for code flow (accurate results when everything is run sequentially)\n",
        "* **5 points** for additional style guidelines listed below\n",
        "\n",
        "Additional style guidelines:\n",
        "* Make sure to rename your .ipynb file to include your netid in the file name: `netid-hw2.ipynb`\n",
        "* For any TODO cell, make sure to include that cell's output in the .ipynb file that you submit. Many text editors have an option to clear cell outputs which is useful for getting a blank slate and running everything beginning-to-end, but always be sure to run the notebook before submitting and ensure that every cell has an output.\n",
        "* When displaying DataFrames, please do not include `.head()` or `.tail()` calls unless asked to. Just removing these calls will work as well, and will allow us to see both the beginning and end of your DataFrames, which help us ensure data is processed properly. Notebooks will by default show only the beginning and end, so you don't have to worry about long outputs here.\n",
        "* If column names are specified in the question, please use the specified name, and please avoid any sorting not specified in the instructions.\n",
        "* For plots, please ensure you have included axis labels, legends, and titles.\n",
        "* To format your short answer responses nicely, we recommend either **bolding** or *italicizing* your answer, or formatting it ```as a code block```.\n",
        "* Generally, please keep your notebook cells to one solution per cell, and preserve the order of the questions asked.\n",
        "* Finally, this can be harder to check/control and depends on which plotting libraries you prefer, but it would be helpful to limit the size/resolution of plot images in the notebook. Our grading platform has an upper limit on submission sizes it can display, and high-res plots are the usual culprit when submissions are hidden or truncated."
      ],
      "metadata": {
        "id": "9C5K6nSqF6oY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ],
      "metadata": {
        "id": "wlsjeVwmGCY7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we need to import some basic libraries that are necessary to complete the assignment."
      ],
      "metadata": {
        "id": "SnFwTlD_GGHd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cg0zxUh-FPpm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add additional modules/libraries to import here (rather than wherever you first use them below):"
      ],
      "metadata": {
        "id": "XVIUdS2GGLic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# additional modules/libraries to import\n"
      ],
      "metadata": {
        "id": "M8uUETcCGP_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We provide some code to get the data files for this assignment into your workspace below. You only need to do the following steps of placing the homework files in your Google Drive once:\n",
        "1. Go to 'My Drive' in your own Google Drive\n",
        "2. Make a new folder named `comp341`\n",
        "3. From the [Google Drive link](https://drive.google.com/drive/folders/1ZEFiqs69jX1ZHtoKeZ2vYFeeHw_HQ1s-?usp=share_link), you can right click the `comp341-hw2` title, and select `Organize -> Add shortcut`, then find and select your `comp341` folder. This is a convenient alternative to having to download and re-upload the files to your own drive.\n",
        "\n",
        "If you run into trouble with accessing the files from the shortcut, then:\n",
        "\n",
        "4. Download `rotten_tomatoes-critics_reviews.csv` and `rotten_tomatoes-movies.csv` to your computer.\n",
        "5. In the `comp341` folder you created in step 2, click `New -> File Upload` and upload the downloaded files from your computer."
      ],
      "metadata": {
        "id": "rg1Tgoh0GXWQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will mount your local Google Drive in colab so that you can read the file in (you will need to do this each time your runtime restarts)."
      ],
      "metadata": {
        "id": "V4mWfM98HL1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# note that this command will trigger a request from google to allow colab\n",
        "# to access your files: you will need to accept the terms in order to access\n",
        "# the files this way (and unfortunately, it seems that Google requires accepting\n",
        "# basically all of the services, even those that seem unrelated like Google photos...\n",
        "# if you prefer not to mount your Google Drive to colab, you are welcome to use\n",
        "# steps 4 and 5 above, where you upload files from your computer to colab every time\n",
        "# you work on the assignment)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# if you followed the instructions above exactly, each file should be\n",
        "# in comp341/; if your files are in a different directory\n",
        "# on your Google Drive, you will need to change the path below accordingly\n",
        "DATADIR = '/content/drive/My Drive/comp341/comp341-hw2'"
      ],
      "metadata": {
        "id": "OY9yiM6YHOOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following command will check the contents of `DATADIR` to double check that you have access to the files. You can now use this file path to read in the data files for the assignment."
      ],
      "metadata": {
        "id": "Il35h2eXYF-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"$DATADIR\""
      ],
      "metadata": {
        "id": "Clk9He5pU0kt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 0: Exploring the Rotten Tomatoes Data [18 pts]"
      ],
      "metadata": {
        "id": "d3LEvl0Wax1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: load the following Rotten Tomatoes files into pandas DataFrames with the following names:\n",
        "# df_movies: \"rotten_tomatoes-movies.csv\"\n",
        "# df_reviews: \"rotten_tomatoes-critics_reviews.csv\"\n",
        "# (note that they are slightly larger files so this will take a little bit of time,\n",
        "# but typically no longer than 30 seconds) [1 pt]\n"
      ],
      "metadata": {
        "id": "kT4jQal62wX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will do some initial exploration and checks for the files, building up towards understanding how many unique movies and unique critics are represented in our dataset."
      ],
      "metadata": {
        "id": "63Q_n2Kv6m7q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will first explore `df_movies` by peeking into the dataset and checking its dimensionality. [1 pt]"
      ],
      "metadata": {
        "id": "-P4RRC7572z-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: check the head of df_movies\n"
      ],
      "metadata": {
        "id": "9vUe_we88JWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: check the shape of df_movies\n"
      ],
      "metadata": {
        "id": "clRl3hNB7zSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Short Answer Question:** Based on this initial look into the head and shape  of the DataFrame, what are some of your initial observations in terms of potential steps that we might need to take to wrangle this dataset into one that is tidy and easy to use for downstream analyses? For example, comment on the `releaseDateStreaming` and `writer` columns. [2 pts]"
      ],
      "metadata": {
        "id": "x2o2wtfM8ycd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`[WRITE YOUR ANSWER HERE]`\n",
        "\n"
      ],
      "metadata": {
        "id": "FTkjm7Ss-Fdu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is nice to see that both `df_movies` and `df_reviews` seems to have a matched `id` column that seems to be a unique identifier for each movie. One would assume that the movie ID should be unique in `df_movies`, but spoiler, this column is not unique."
      ],
      "metadata": {
        "id": "5KCXRlmk-piS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How many duplicated movie ids are there?"
      ],
      "metadata": {
        "id": "5W5b6JhH_Nvm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: calculate the number of duplicates in the `id` column [1 pt]\n"
      ],
      "metadata": {
        "id": "kUHof1TT-4zC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How many completely duplicated rows are there?"
      ],
      "metadata": {
        "id": "pODg16C2MpYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: calculate the number of completely duplicated rows in your df_movies DataFrame [1 pt]\n"
      ],
      "metadata": {
        "id": "2l1HqKyyMsO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you calculated everything correctly, you'll discover that these two numbers are close, but not the same! We want to take a look at which movies have the same `id` but differences in other fields."
      ],
      "metadata": {
        "id": "5ze1BCAXNKlf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: find and examine the rows in df_movies that have identical `id`s but differences in other fields [1 pt]\n"
      ],
      "metadata": {
        "id": "SARA_4NmNTA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Examining `df_movies` in this way helps reveal that the additional duplicate `id`s are probably more *spurious*, meaning that they do not represent a truly independent, different observation in our data. As such, we should feel comfortable about removing the duplicates and *spurious* entries that have the same `id`."
      ],
      "metadata": {
        "id": "ffh9oDlGOY9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: update df_movies to remove all duplicates so that the new DataFrame has only unique ids,\n",
        "# making sure that for any observations that had the same id but differences in other fields,\n",
        "# the more reasonable entry is kept [1 pt]\n"
      ],
      "metadata": {
        "id": "ymyQgLImPe0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, how many unique movies based on their Rotten Tomatoes `id` are in this dataset?"
      ],
      "metadata": {
        "id": "Gek5HgC4QHfZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: calculate the number of unique movies based on `id` in df_movies [1 pt]\n"
      ],
      "metadata": {
        "id": "_-m-aB_QQYBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We know that movies can sometimes have the same title. Calculate the number of unique titles in this dataset, the top 5 most popular titles, and how many times each of those 5 titles have appeared in our dataset."
      ],
      "metadata": {
        "id": "XO1Wy15RQovs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: calculate the number of unique titles in df_movies [1 pt]\n"
      ],
      "metadata": {
        "id": "8ebNLbS_Ra0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: calculate the top 5 most popular titles and\n",
        "# show a table of the titles together with how many times they appear in df_movies [1 pt]\n"
      ],
      "metadata": {
        "id": "vEeBe36hRjFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be revisiting `df_movies` for a deeper data cleaning effort in Part 1.\n",
        "\n",
        "In the mean time, we move on to our other data, `df_reviews`. You are free to do some initial exploration to get you oriented to the DataFrame (ungraded)."
      ],
      "metadata": {
        "id": "1eLs0CWDD8jJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# optional exploration for df_reviews\n"
      ],
      "metadata": {
        "id": "wynZDms9Jb1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you did some initial exploration of `df_reviews`, you would quickly see that there might be some missing values. Which columns in `df_reviews` have missing values, and how many?"
      ],
      "metadata": {
        "id": "FMsbP0UCEWZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: examine which columns in df_reviews have missing values, and when there are missing values, how many [1 pt]\n"
      ],
      "metadata": {
        "id": "ECUIf10LEW7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Short Answer Question:** In looking at both the column names and the contents of `df_reviews`, it is interesting to see that there is a column for the famous \"fresh vs rotten\" `reviewState`, and also a `scoreSentiment` that captures the sentiment of each review. Do these two columns are redundant or capture differing information? [1 pt]"
      ],
      "metadata": {
        "id": "VVlEMqiWEfK3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: explore how `reviewState` and `scoreSentiment` may be similar or different\n"
      ],
      "metadata": {
        "id": "3qwLzqUxEimG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`[WRITE YOUR ANSWER HERE]`"
      ],
      "metadata": {
        "id": "NNlS3HfVVmAj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The number of movies a critic has reviewed has the possibility to vary greatly. We would like to focus on critics that have ranked many movies, but to do that, we have to have a good sense of the *distribution* of unique movie reviews each critic has written. We will be using the `originalScore` column later, so want to only consider reviews that report an `originalScore`.\n",
        "\n",
        "Doing this analysis will both tell us how many unique critics there are in this dataset and give us a sense of what might be a good \"threshold\" for a prolific reviewer."
      ],
      "metadata": {
        "id": "YJ8rTqATWoMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: calculate the number of unique movie reviews per critic\n",
        "# (considering only movie reviews that include an originalScore) and\n",
        "# output the number of unique movie critics represented in this subset of the data [1 pt]\n"
      ],
      "metadata": {
        "id": "wwNHjXmaXKmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: visualize the distribution of values for the unique review count [1 pt]\n"
      ],
      "metadata": {
        "id": "xYR7QgrpV7ve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You may have noticed that there is a wide range in how many reviews a critic may write. Our goal is to determine a rough threshold for a \"somewhat prolific\" critic. There is no single right answer, but explore the data some more to choose an appropriate threshold (your exploration can include summary statistics or more plots)."
      ],
      "metadata": {
        "id": "eFu91LDkWbw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: explore the # of unique reviews across critics some more to\n",
        "# suggest a potential threshold for prolific reviewers. [2 pts]\n"
      ],
      "metadata": {
        "id": "3PxwnHs0WzRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Short Answer Question:** Motivate how you chose your threshold and what might be important factors to consider when choosing this threshold. [1 pt]"
      ],
      "metadata": {
        "id": "gjVQPb_rYjdN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`[WRITE YOUR ANSWER HERE]`"
      ],
      "metadata": {
        "id": "R5-ZaRIVYu80"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 1: Reviewer Bias [18 pts]\n",
        "\n",
        "Some critics might tend to always say movies are terrible and some might just love movies too much and think all movies are great. Let's look at the fresh/rotten categories and check for trends per reviewer (using the filtered set of critics that you have selected from Part 0)."
      ],
      "metadata": {
        "id": "GxfI6zZaZYzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: using reviewState, determine the number of fresh vs rotten reviews per critic\n",
        "# if the critic passes your chosen threshold for a \"prolific\" critic\n",
        "# your dataFrame should include criticName, the # of fresh reviews, and the # of rotten reviews [2 pts]\n"
      ],
      "metadata": {
        "id": "MAaXlvOKZqp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: plot the distribution of percentages of fresh reviews per critic [2 pts]\n"
      ],
      "metadata": {
        "id": "iAwaoCPEaUZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Short Answer Question:** Would you say that critics are more likely to write a negative or positive movie review? [1 pt]"
      ],
      "metadata": {
        "id": "zeypZe40anky"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`[WRITE YOUR ANSWER HERE]`"
      ],
      "metadata": {
        "id": "2a49q7lCarNH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we would like to add another layer to our exploration of critic preferences by analyzing critic preferences by movie genre. However, the `df_reviews` DataFrame only has a movie `id`, so we will need to clean and extract that information from `df_movies`. Unfortunately, it seems that the `genre` column is not very tidy."
      ],
      "metadata": {
        "id": "PWesTf6AZeTv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: tidy up the genre column in `df_movies` so that individual genres\n",
        "# are represented as separate rows (it is okay if all other elements in the DataFrame\n",
        "# are repeated) [2 pts]\n"
      ],
      "metadata": {
        "id": "cXxS6WMF7b6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: create a new `df_movies_sub` DataFrame that only includes movies that\n",
        "# do not have missing values for audienceScore, genre, tomatoMeter, and boxOffice [1 pt]\n"
      ],
      "metadata": {
        "id": "1zRTwhx_-YHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: calculate and display each unique genre represented in `df_movies_sub`\n",
        "# together with how many movies of that genre are in the dataset [1 pt]\n"
      ],
      "metadata": {
        "id": "I0RpFb0f-hPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seeing the specific values should immediately suggest that there are some string operations we can do on `genre` to clean them up."
      ],
      "metadata": {
        "id": "1YPr8HeibXug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: use string operations in pandas to clean up the genre column and\n",
        "# display the updated, cleaner table of genres + number of movies in each genre [2 pts]\n"
      ],
      "metadata": {
        "id": "JvZtLIM--9q_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Short Answer Question:** Are there categories that would make intuitive sense to combine? If so, which ones? [1 pts]"
      ],
      "metadata": {
        "id": "xP8HCp4noA3H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`[WRITE YOUR ANSWER HERE]`"
      ],
      "metadata": {
        "id": "yCy0K7VaoMpC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: for any genres with fewer than 500 movies, merge any reasonable groupings into existing genres that\n",
        "# can reasonably be combined together in df_movies_sub (using the name from the more popular genre name),\n",
        "# then filter df_movies_sub to only include movies in genres that have at least 500 unique movies\n",
        "# and only includes the following columns:\n",
        "# id, title, audienceScore, tomatoMeter, rating, genre, boxOffice\n",
        "# display the resulting DataFrame [2 pts]\n"
      ],
      "metadata": {
        "id": "0Sdz7GEfoPlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, use your data exploration and summarizing skills to try and answer our original question of whether critics may be biased towards or against certain genres. For the purposes of this analysis, you only need to focus on the 5 genres with the most movies.\n",
        "\n",
        "There is no single right answer, as this question is naturally more open-ended, but in general, you will be evaluated on appropriate use of data visualization and/or summary statistics to look for genre-specific bias."
      ],
      "metadata": {
        "id": "uX3pGCciLFaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: for the top 5 most popular movie genres, explore whether critics might have biases (for or against) [2 pts]\n"
      ],
      "metadata": {
        "id": "Lcjtm4McLE1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Short Answer Question:** What did you conclude from your data exploration regarding genre bias? [2 pts]"
      ],
      "metadata": {
        "id": "_Z1_vEPvLzAA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`[WRITE YOUR ANSWER HERE]`"
      ],
      "metadata": {
        "id": "wIhf8qGKL20k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 2: Reviews and Box Office [20 pts]"
      ],
      "metadata": {
        "id": "ANLiZa3YMgFn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are interested in whether there is any interesting \"structure\" or clusters in our movie dataset based on critics' reviews (as captured by the Tomato Meter) and box office. To explore this question a little bit more, we will first simplify our dataset to only look at \"simpler\" movies (as defined by how many genres they're associated with) and only of a few more \"clearly\" defined genres."
      ],
      "metadata": {
        "id": "BIGiNI5jMoOr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: create a new df_movies_sub_single_genre DataFrame that further subsets your df_movies_sub DataFrame\n",
        "# to include movies that have only been assigned a single genre\n",
        "# then, filter this DataFrame even more to include represent the following 5 genres: [1 pt]\n",
        "#  Action, Adventure, Documentary, Fantasy, Romance\n"
      ],
      "metadata": {
        "id": "ypL7S-W_gh2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visual inspection of the `boxOffice` feature will reveal that these values are various abbreviations for how much money the movie made and are not numeric. This means we will have to do some data cleaning if we want to compare these box office numbers."
      ],
      "metadata": {
        "id": "8N_-pgpDOpXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: write a helper function that can convert the boxOffice values in your\n",
        "# df_movies_sub_single_genre DataFrame to the amount of money the movie made (in millions)\n",
        "# (e.g., $2.3M should be represented as 2.3, whereas $3.4B would be represented as 3400) [3 pts]\n"
      ],
      "metadata": {
        "id": "FazFYqQGjHys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: using your new helper function, convert the boxOffice values now to their numeric counterpart [1 pt]\n"
      ],
      "metadata": {
        "id": "5xy8WQDjm6g5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: visualize the relationship between tomatoMeter and boxOffice, where each movie is a point\n",
        "# Note: since the box office spans several orders of magnitude, the y-axis should be on a logarithmic\n",
        "# scale (base 10 for the best interpretability) [2 pts]\n"
      ],
      "metadata": {
        "id": "t6JGP1ESf_k5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see if we can use clustering to help make sense of any potential groupings of movies here. We will use k-means, but as discussed in class, how to choose k is often challenging. Before any further exploration, note how what k you would guess given what you observe in the figure.\n",
        "\n",
        "We will use the elbow plot, silhouette score, and adjusted Rand index to help us choose k. For adjusted Rand index, we do not have any \"true clusters,\" but we can try and see if the `genre` could help act as a proxy for similarity here."
      ],
      "metadata": {
        "id": "A-24T9JAQ8H0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: for k from 1 to 10, calculate the inertia and adjusted Rand index (using genre as your \"labels\")\n",
        "# as silhouette score requires at least 2 clusters, you can calculate it for k from 2 to 10 (excluding the k=1 case) [3 pts]\n"
      ],
      "metadata": {
        "id": "EvkCVyjW2dMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: plot the elbow plot for your k-means clustering results [1 pt]\n"
      ],
      "metadata": {
        "id": "XNZrRi4e5l9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: visualize how average silhouette changes with k [1 pt]\n"
      ],
      "metadata": {
        "id": "MvR6oisz8tbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: plot how the adjusted Rand index changes with k [1 pt]\n"
      ],
      "metadata": {
        "id": "JzbvgZeEBCB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Short Answer Question:** Based on the different metrics you calculated, what value of k would you choose? How similar or different is it from what you thought before running k-means? [2 pts]"
      ],
      "metadata": {
        "id": "TzSU_pON9SA7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`[WRITE YOUR ANSWER HERE]`"
      ],
      "metadata": {
        "id": "946FBpK9U03E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: run k-means one more time using your chosen k and get the cluster labels for each movie [1 pt]\n"
      ],
      "metadata": {
        "id": "eu7CL4v_9Th1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: build upon your previous visualization for the relationship between tomatoMeter and boxOffice,\n",
        "# where each movie is a point, now coloring the points by cluster label\n",
        "# Recall: since the box office spans several orders of magnitude, the y-axis should be on a logarithmic\n",
        "# scale (base 10 for the best interpretability)\n",
        "# Additionally, think about what type the cluster is and whether the color choice matches [1 pt]\n"
      ],
      "metadata": {
        "id": "bN6IYN0x9cjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: now, instead of coloring by cluster label, color each point by genre\n",
        "# (keeping everything else about the figure the same) [1 pt]\n"
      ],
      "metadata": {
        "id": "m_VZjh2cBUyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Short Answer Question:** What is your interpretation for how k-means ended up clustering your data? Does this have any correspondence with where movies of each genre are in the same figure? [2 pts]"
      ],
      "metadata": {
        "id": "-aL0GvmoWv51"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`[WRITE YOUR ANSWER HERE]`"
      ],
      "metadata": {
        "id": "1onJFGrfWwVY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 3: Diving Deeper into Data Cleaning [15 pts]\n",
        "\n",
        "Critics do not grade movies on the same scale. To make them comparable, we will standardize the scale to 0-100 using the existing scores.\n",
        "\n",
        "For the subsequent analyses, we will be using a critic filter of at least 500 movies (which may be different from the cutoff and analyses you did in Part 1) so that everyone has a consistent analysis."
      ],
      "metadata": {
        "id": "MixMnR7SEiDU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: add a critic filter to your original df_reviews DataFrame,\n",
        "# keeping only critics that have reviewed at least 500 movies,\n",
        "# and keep only the following columns: id, reviewId, criticName, originalScore, reviewState, publicationName\n",
        "# store this a DataFrame named `df_reviews_prolific` and display it [1 pt]\n"
      ],
      "metadata": {
        "id": "0sXXJ-dGbVFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A quick glance at the values in the `originalScore` column even in this smaller DataFrame you have constructed will show that critics like to use vastly different scoring methods, unfortunately much messier than the `boxOffice` feature you cleaned earlier. You now have the painful task of standardizing these scores to a 0-100 scale. Write functions to help you clean `originalScore` and store the standardized values in a `standardizedScore` column in your `df_reviews_prolific` DataFrame."
      ],
      "metadata": {
        "id": "vhhFo1k2eQDt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: write helper functions to help guide your score conversion here\n",
        "# includes comments to explain what you are doing; for situations where\n",
        "# it is unclear to you how the results should map to a 0-100 scale,\n",
        "# return np.nan\n",
        "# (hint: it may be easier to write and debug if you separate functions\n",
        "# for different \"scoring standards,\" and this will likely involve\n",
        "# a lot of iteration to cover as many cases as you can) [8 pts]\n"
      ],
      "metadata": {
        "id": "9dIwDfjCc31E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: using your helper functions from above, convert as many originalScore values as you can in a reasonable,\n",
        "# systematic manner to fall between 0 and 100 and display your updated df_reviews_prolific [2 pts]\n"
      ],
      "metadata": {
        "id": "AqyyWh5Pfr64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: show the that the values fall between 0 and 100 using describe()\n",
        "# on your `standardizedScore` column [1 pt]\n"
      ],
      "metadata": {
        "id": "NGGG8UbmiKWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Short Answer Question:** How did you approach this score conversion task? Specifically, what different \"scoring schemes\" did you infer different critics were using and how did you address them? How many reviews were you unable to standardize? [2 pts]"
      ],
      "metadata": {
        "id": "CUfBQzd4nfcJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`[WRITE YOUR ANSWER HERE]`"
      ],
      "metadata": {
        "id": "MUzAnaGtnyoX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: examine your df_reviews_prolific DataFrame and do some final cleaning steps,\n",
        "# removing any reviews where you could not find a good way to convert the originalScore\n",
        "# and any completely duplicated rows [1 pt]\n"
      ],
      "metadata": {
        "id": "1opHbSRFympy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 4: Audience vs Critics [14 pts]\n",
        "\n",
        "We now can tackle the question of identifying critics that give reviews that correlate well with the audience score. Unfortunately, critics do not review every movie. Even if we restrict critics to those that have reviewed at least 500 movies (as in part 3 above), there is likely only a very small set of total of movies that every critic has reviewed. This means there will be many missing values in the data. Fortunately, there are some tricks we can use to fill in the gaps. In this section we will try 3 different methods: 0, mean, KNN impute."
      ],
      "metadata": {
        "id": "2U09GxcNrnz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, to calculate more accurate correlations between audience and critics, make a new DataFrame, `df_reviews_prolific_sub` based off of `df_reviews_prolific`, but only keeping movies that have greater than 50 reviews."
      ],
      "metadata": {
        "id": "_8uCpuXJe36l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: make a df_reviews_prolific_sub that keeps only movies that have greater than 50 reviews from df_reviews_prolific [1 pt]\n"
      ],
      "metadata": {
        "id": "Y0nZUErCG8Wg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: combine df_movies_sub and df_reviews_prolific_sub in a df_movie_critics DataFrame\n",
        "# that only keeps overlapping entries (i.e., each row in your merged DataFrame will have both movie\n",
        "# information as well as critic information)\n",
        "# examine df_movie_critics [1 pt]\n"
      ],
      "metadata": {
        "id": "Vbc2M7Gaqodr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`df_movie_critics` is a \"long\" DataFrame, but we will want to try to estimate correlations between critics and audience across all the movies left in this smaller dataset, so the first thing we want to do is reshape `df_movie_critics` to a wide, matrix-like DataFrame, where movie ids are rows and critics are columns, with each entry in the matrix being either a value from `standardizedScore` or a missing value (meaning either the critic did not review the movie or you were unable to estimate a good `standardizedScore` given the critic's original score earlier)."
      ],
      "metadata": {
        "id": "lrUfYpmWfvHY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: reshape df_movie_critics into a matrix of standardizedScores [2 pts]\n"
      ],
      "metadata": {
        "id": "7IpJLkzb0F_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Short Answer Question:** This short answer question also serves as a hint for the reshaping task. You might notice that the matrix-conversion is not completely straightforward due to certain types of duplicates in your data. How did you address different types of duplicates during the data reshaping process? [1 pt]"
      ],
      "metadata": {
        "id": "iX2EsJ6sgwpI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`[WRITE YOUR ANSWER HERE]`"
      ],
      "metadata": {
        "id": "V30aG_8PhN72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: fill any missing values in your matrix of standardizedScores with 0 [1 pt]\n",
        "# suggestion: you will need to try other missing value estimation ways in the next steps,\n",
        "# so it is better if you make a new DataFrame that has the 0s filled in\n"
      ],
      "metadata": {
        "id": "8Ecu-9pb4vaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: extract audienceScore for the same movies, ensuring that the ordering of values\n",
        "# also matches the movie ordering in your 0-imputed matrix [1 pt]\n"
      ],
      "metadata": {
        "id": "pm9PqHUYvXAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: who are the top 10 critics with standardizedScore most correlated with 0-imputed audienceScore? [1 pt]\n"
      ],
      "metadata": {
        "id": "fi79dj_auxRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: now, for each critic's missing scores, fill them in with the critic's average review score [1 pts]\n"
      ],
      "metadata": {
        "id": "6hyGkvuY6eE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: who are the top 10 critics with standardizedScore most correlated with mean-imputed audienceScore? [1 pt]\n"
      ],
      "metadata": {
        "id": "xCisWChq5FaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: finally, fill in missing values using KNN imputation, with K=6\n",
        "# note that even though your dataset is now a subset of the original,\n",
        "# KNNImputer is much slower than the previous 2 methods and will take some time to run,\n",
        "# but it should be no longer than 3 minutes [2 pts]\n"
      ],
      "metadata": {
        "id": "_9Q5hp329_KQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: who are the top 10 critics with standardizedScore most correlated with KNN-imputed audienceScore? [1 pt]\n"
      ],
      "metadata": {
        "id": "A43jyJ_ILebd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Short Answer Question:** Compare the top 10 critics identified using the 3 different imputation methods. Did you expect them to be the same or all different? What does this say about the choice of imputation method? [1 pt]"
      ],
      "metadata": {
        "id": "Vut8T8uhQjp6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`[WRITE YOUR ANSWER HERE]`"
      ],
      "metadata": {
        "id": "ZOp3rtXlQrF5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bonus section [up to 10 pts extra credit]\n",
        "\n",
        "There have been observations that movies, especially action movies, might prefer to have hero names that start with 'J' (see [\"Why are All Action Heroes Named Jack, James, or John?\"](https://slate.com/culture/2023/03/john-wick-james-bond-action-heroes-j-names.html)), though like we saw in HW1, these trends also change over time.\n",
        "\n",
        "While we unfortunately do not have the names of the main characters in our Rotten Tomatoes dataset so that we can easily investigate this pattern, many movies do directly use names in their titles. Given our baby names dataset from HW1, we can at least do some investigations to see if this pattern holds in movie titles.\n",
        "\n",
        "For all movies in our dataset that made over $1M at the box office and all baby names that have been used at least 50k times in the history of the social security database we used in HW1, investigate the following questions and answer them using a combination of your analyses and appropriate data visualizations:\n",
        "- how many movie titles have used one of these names? [+2 pts]\n",
        "- vice versa, how many names have been used in movie titles? what are some of the most popular names (with example movie titles)? are there sex biases in the names most frequently used in movie titles? [+3 pts]\n",
        "- are there biases for the 'starting' letter as suggested by the linked article? is this something that correlates with movie genre? is the proportion very different than you'd expect based on baseline differences in 'starting' letters of popular names? specifically, do names that start with 'J' really appear more frequently in action movies than you'd expect based on how popular 'J' names are versus names that start with other letters? [+5 pts]"
      ],
      "metadata": {
        "id": "rFVlEMvkjWQP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## To Submit\n",
        "Download the notebook from Colab as a `.ipynb` notebook (`File > Download > Download .ipynb`), rename it to `netid-hw2.ipynb`, and upload it to the corresponding Gradescope assignment."
      ],
      "metadata": {
        "id": "yeljgUCWjerq"
      }
    }
  ]
}